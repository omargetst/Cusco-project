# Cusco-project
# ğŸ§  Asistente con Ollama â€“ Interfaz de Chat Multimodal

Este proyecto proporciona una interfaz interactiva construida con [Streamlit](https://streamlit.io) que se conecta con el modelo LLM de **Ollama**. El asistente puede recibir entradas de texto, leer documentos PDF e interpretar imÃ¡genes para ofrecer respuestas contextuales y Ãºtiles.

---

## ğŸš€ CaracterÃ­sticas principales

- âœ… **Chat en lenguaje natural** con modelo de lenguaje vÃ­a Ollama
- ğŸ“ **Carga de archivos PDF o imÃ¡genes** como contexto adicional (JPG, PNG)
- ğŸ’¬ **Historial de mensajes** y copia rÃ¡pida de respuestas
- ğŸ¨ **DiseÃ±o moderno y responsivo**
- ğŸŒ™ **Modo centrado o ancho** intercambiable
- ğŸ§  **Respuestas enriquecidas** con Markdown (listas, cÃ³digo, etc.)
- ğŸ“‹ **BotÃ³n de copiar respuesta al portapapeles**
- ğŸ–¥ï¸ Listo para ejecutar en [Streamlit Cloud](https://streamlit.io/cloud)

---

## ğŸ› ï¸ Estructura del proyecto

asistente-ollama/
â”‚
â”œâ”€â”€ app.py â† archivo principal de la app Streamlit
â”œâ”€â”€ requirements.txt â† dependencias para ejecutarla
â”œâ”€â”€ README.md â† documentaciÃ³n del proyecto
â””â”€â”€ chat/
â”œâ”€â”€ init.py
â”œâ”€â”€ ollama_api.py â† manejo de llamadas a Ollama
â”œâ”€â”€ extractors.py â† OCR y lectura de PDF o imÃ¡genes
â”œâ”€â”€ state.py â† manejo de sesiÃ³n
â””â”€â”€ utils.py â† utilidades (copiado, estilos, etc.)
